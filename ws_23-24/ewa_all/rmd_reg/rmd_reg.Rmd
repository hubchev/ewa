---
title             : "Weighting, data management and regression"
shorttitle        : "Weighting, data management and regression"

author: 
  - name          : "Prof. Dr. Stephan Huber"
    affiliation   : "1,2"
    corresponding : no    # Define only one corresponding author
    address       : "Im Mediapark 4e"
    email         : "stephan.huber@hs-fresenius.de"

affiliation:
  - id            : "1"
    institution   : "Fresenius University of Applied Science"
  - id            : "2"
    institution   : "Charlotte Fresenius University"

authornote: |
  All files related to this document can be found here: [https://github.com/hubchev/ewa](https://github.com/hubchev/ewa). Please contact us via `stephan.huber@hs-fresenius.de`.

abstract: |
  In this document I show what weighted means and their distribution are all about. Furthermore, I show some possibilities of data management in R with the `dplyr` package and how a regression analysis in R is performed and visualised.

bibliography      : "lit.bib"

floatsintext      : no
linenumbers       : no
draft             : no
mask              : no
numbersections    : yes

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "doc"
output            : papaja::apa6_pdf 
toc               : no
csl               : "apa.csl"
---


```{r, include=FALSE}
if (!require(pacman)) install.packages("pacman")
pacman::p_load(tinylabels, 
               papaja,
               haven, 
               sjlabelled,
               expss,
               psych,
               tidyverse, 
               rstatix, 
               ggstats, 
               stargazer,
               texreg, 
               sjPlot
)
```

\newpage
# Solutions and Cheatsheet

Please consider the report you find here:

[https://hubchev.github.io/various/exam_functions.html](https://hubchev.github.io/various/exam_functions.html)

In this report, I summarize operators and popular functions of R. Moreover, I present the output of all exercises. That should help you to write code and start to look for solutions to your challenges in working with data:

# Datenmanagement

```{r data1, echo=FALSE}
library("tidyverse")
rm(list = ls())
df <- tibble(
  v1 = c(1, 2, "", 4, 5),
  v2 = c("A", NA, "C", "D", "E"),
  v3 = c(NA, 0.2, 0.3, NA, 0.5),
  v4 = c("", 0.4, 0.1, 3, 1.5)
)

knitr::kable(df, "latex", caption = "Data")
``` 

Consider the data of Table \@ref(tab:data1) and solve the following exercises:

a) Add variable `misone` that is 1 if there is a missing and 0 otherwise.
_(Hint: Use `case_when` and `is.na()`.)_

\newpage


```{r, echo=T}
df <- df |> 
  mutate(misone = case_when(
    v1 == "" | is.na(v1) ~ 1,
    v2 == "" | is.na(v2) ~ 1,
    v3 == "" | is.na(v3) ~ 1,
    v4 == "" | is.na(v4) ~ 1,
    TRUE ~ 0 
  ))

knitr::kable(df, "latex", caption = "Solution a)")
```

\newpage

b) Add variable `miscount` that counts how many observations are missing in each row.
_(Hint: Use  `mutate_all`, `rowSums`, and  `pick(everything())`)_

\newpage

```{r, echo=T} 
test_df <- df |> 
  mutate_all(~ if_else(is.na(.) | . == "", 1, 0)) |>
  mutate(miscount = rowSums(pick(everything())))

test_df_miscount <- test_df |> 
  select(miscount) 
  
df <- bind_cols(df, test_df_miscount)

knitr::kable(df, "latex", caption = "Solution b)")
``` 

\newpage
c) Use the function `rowwise` to calculate the `NA` and `""` observations.
_(Hint: Use `is.na` and `pick(everything())`.)_
 
\newpage
```{r, echo=T} 
df <- df |> 
  rowwise() |> 
  mutate(count_NA = sum(is.na(pick(everything())))) |> 
  mutate(count_OK = sum(pick(everything()) == "", na.rm = TRUE)) |> 
  ungroup()

knitr::kable(df, "latex", caption = "Solution c)")
``` 


\newpage

d) Add variable `mispercent` that measures the percentage of missings and a variable mis30up that is 1 if the percentage is above 30%. _(Hint: Use `mutate`, `select`, `ifelse`, and `bind_cols`.)_

\newpage
```{r, echo=T} 
test_df_mis30up <- test_df |> 
  mutate(fraction = miscount / 4) |> 
  mutate(mis30up = ifelse(fraction > 0.3, 1, 0)) |> 
  select(mis30up, fraction)
  
df <- bind_cols(df, test_df_mis30up)

knitr::kable(df, "latex", caption = "Solution d)")
``` 

\newpage
e) Calculate the average of the numeric variables `v1`, `v3`, and `v4`. Name the variable `average`. 
_(Hint: Use  `as.numeric`, `rowwise`, and `mean`.)_

\newpage
```{r, echo=T} 
df <- df |> 
  mutate(
    v1 = as.numeric(v1),
    v4 = as.numeric(v4)
  )
df <- df |> 
  rowwise() |> 
  mutate(average = mean(c(v1, v3, v4), na.rm = TRUE)) |> 
  ungroup()

test_df <- df |> 
  select(v1, v3, v4, average)

knitr::kable(test_df, "latex", caption = "Solution e)")
```

\newpage
# Regression

Please consider my lecture notes concerning __Regression Analysis__ which you find here:

[https://hubchev.github.io/qm/statistics.html#simple-linear-regression](https://hubchev.github.io/qm/statistics.html#simple-linear-regression)

Moreover, I highly recommend reading @Wysocki2022Statistical which is freely available here: [https://journals.sagepub.com/doi/10.1177/25152459221095823](https://journals.sagepub.com/doi/10.1177/25152459221095823). They explain how difficult it is to use regression analysis to dentify a causal impact. The main insights of the paper are nicely summarized here: [https://osf.io/38mxq](https://osf.io/38mxq).

## Making regression tables using `apa_table`

Here is an example how to use `apa_table` from the `papaja` package to make regression output tables.

```{r, echo=T} 
# Load the mtcars dataset
data("mtcars")

# Fit a linear regression model
m1 <- lm(mpg ~ wt + hp, data = mtcars)
m2 <- lm(mpg ~ wt , data = mtcars)

# Summary of the model
summary(m1)

apa_lm <- apa_print(m1)
apa_table(
  apa_lm$table
  , caption = "A full regression table."
)
```



\newpage
# Example 


## Data
In the statistic course of WS 2020, I asked 23 students about their weight, height, sex, and number of siblings:

```{r, echo = TRUE}
library("haven")
classdata <- read.csv("https://raw.githubusercontent.com/hubchev/courses/main/dta/classdata.csv")

head(classdata)
```


\newpage
## First look at data

```{r pressure, echo=TRUE}
library("ggplot2")
ggplot(classdata, aes(x=height, y=weight)) + geom_point() 

```

\newpage
## Include a regression line:

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight)) +
  geom_point() +
  stat_smooth(formula=y~x, method="lm", se=FALSE, colour="red", linetype=1)

```

\newpage
## Regression: Distinguish male/female by including a seperate constant:

```{r , echo=TRUE}
## baseline regression  model
model  <- lm(weight ~ height + sex , data = classdata )
show(model)
interm <- model$coefficients[1] 
slope  <- model$coefficients[2]
interw <- model$coefficients[1]+model$coefficients[3] 
```


```{r, echo=TRUE}
summary(model)
```

\newpage
```{r, echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point() +
  geom_abline(slope = slope, intercept = interw, linetype = 2, size=1.5)+
  geom_abline(slope = slope, intercept = interm, linetype = 2, size=1.5) +
  geom_abline(slope = coef(model)[[2]], intercept = coef(model)[[1]]) 

```

\newpage
That does not look good. Maybe we should introduce also different slopes for male and female.


```{r , echo=TRUE}

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  method = "lm", 
              se = FALSE, colour = "red", linetype = 1)

```


\newpage
## Can we use other available variables such as siblings?

```{r , echo=TRUE}
ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = siblings)) 

```

```{r , echo=TRUE}
## baseline model
model  <- lm(weight ~ height + sex , data = classdata )

ggplot(classdata, aes(x=height, y=weight, shape = sex)) +
  geom_point( aes(size = 2)) +
  stat_smooth(formula = y ~ x,  
              method = "lm", 
              se = T, 
              colour = "red", 
              linetype = 1)

```

\newpage
## Let us look at regression output:

```{r, echo=TRUE, results='hide'}

m1 <- lm(weight ~ height , data = classdata )
m2 <- lm(weight ~ height + sex , data = classdata )
m3 <- lm(weight ~ height + sex + height * sex , data = classdata )
m4 <- lm(weight ~ height + sex + height * sex + siblings , data = classdata )
m5 <- lm(weight ~ height + sex + height * sex , data = subset(classdata, siblings < 4 ))

```

\newpage
```{r echo=FALSE, results='asis'}
model.lst = list(m1, m2, m3, m4, m5)

stargazer(m1,
          m2,
          m3,
          m4,
          m5,
          title="Regression",
          type = "latex",
          font.size = "small",
          column.labels = c("Model-1", "Model-2", "Model-3", "Model-4", "Model-5"),
          column.separate = c(1,1, 1, 1,1),
          digits = 2,
          notes.align = "l",
          notes = "Here are my notes.",
          notes.append = TRUE, 
          df = FALSE,
          header=FALSE
          )

```

\newpage
## Interpretation of the results

- We can make predictions about the impact of height on male and female
- As both, the intercept and the slope differs for male and female we should interpret the regressions seperately:
- One centimeter more for **MEN** is *on average* and *ceteris paribus* related with 0.16 kg more weight.
- One centimeter more for **WOMEN** is *on average* and *ceteris paribus* related with 1.01 kg more weight.

## Regression Diagnostics

Linear Regression makes several assumptions about the data, the model assumes that:

- The relationship between the predictor (x) and the dependent variable (y) has linear relationship.
- The residuals are assumed to have a constant variance.
- The residual errors are assumed to be normally distributed.
- Error terms are independent and have zero mean.

More on regression Diagnostics can be found [Applied Statistics with R: 13 Model Diagnostics](https://daviddalpiaz.github.io/appliedstats/model-diagnostics.html#r-markdown-6)


\newpage
# Weighting

The formula for the weighted mean is:
\[
\bar{x} = \frac{\sum_{i=1}^{n} w_i \cdot x_i}{\sum_{i=1}^{n} w_i}
\]

In this formula:

- $\bar{x}$ represents the weighted mean.
- $n$ is the number of observations.
- $w_i$ represents the weight for the $i$-th observation.
- $x_i$ represents the $i$-th observation value.


```{r echo=TRUE}
rm(list = ls())
wt <- c(5, 2, 2, 1)
x <- c(1, 2, 3, 4)
x_mean <- mean(x)
x_mean

x_wt_mean_1 <- weighted.mean(x, wt)
x_wt_mean_1
```


Let us calculate the weighted mean manually:

```{r echo=TRUE}
product <- wt*x
# Nominator
nom <- sum(product)
nom
# Denominator
denom <- sum(wt)
denom

x_wt_mean_2 <- nom/denom
x_wt_mean_2
```

\newpage
## Exercise 1

Below you see an alternative way to calculate the weighted mean. Can you explain it?

```{r echo=TRUE}
w_div_sumw <- wt/denom
w_div_sumw
multi_ww_x <- w_div_sumw * x
multi_ww_x
x_wt_mean_3 <- sum(multi_ww_x)
x_wt_mean_3
```

\newpage
## Exercise 2

a) Calculate mean, variance, weighted mean, and the variance of the weighted mean for `x`.

```{r echo=TRUE}
results <- data.frame(
  Statistic = c("Mean", "Variance", "Weighted Mean", "Weighted Variance"),
  Value = c(mean(x), var(x), weighted.mean(x, wt), sum(wt * (x - weighted.mean(x, wt))^2) / sum(wt))
)
print(results)
```

b) Do it again but use `tidyverse` and the function `summarize`.

```{r, message=FALSE}
if (!require(pacman)) install.packages("pacman")
pacman::p_load(tidyverse)
```
 

```{r, echo=TRUE}
df <- tibble(wt = wt, x = x)

summary_stats <- df %>%
  summarize(
    Mean = mean(x),
    Variance = var(x),
    Weighted_Mean = weighted.mean(x, wt),
    Weighted_Variance = sum(wt * (x - weighted.mean(x, wt))^2) / sum(wt)
  )

# Display the table
print(summary_stats)
``` 


# References
