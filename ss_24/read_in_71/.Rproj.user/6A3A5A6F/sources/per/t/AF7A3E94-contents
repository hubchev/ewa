---
title: "Dokumentation der Datenaufbereitung:"
title-secondline: "'Dataset 71.txt'"
subtitle: "Empririsch Wissenschaftliches Arbeiten"
shorttitle: "Datenaufbereitung: 'Dataset 71.txt'"

author: 
  - name          : "Prof. Dr. Stephan Huber"
    affiliation   : "1,2"
    corresponding : true
    address       : "Im Mediapark 4e"
    email         : "stephan.huber@hs-fresenius.de"

university: "Charlotte Fresenius Hochschule"
studiengang: "Studiengang: Psychologie (B. Sc.)"
place: "Studienort: Köln"

gutachter: "---"
date: today
date-format: "DD.MM.YYYY"


semester-eins: ""
author-zwei: ""
semester-zwei: ""
author-drei: ""
semester-drei: ""
bibliography: literatur.bib
# csl: "https://www.zotero.org/styles/deutsche-gesellschaft-fur-psychologie"
format:
  apaquarto-pdf:
    documentmode: stu
toc: true
floatsintext: false
number-sections: false
a4paper: true
fontsize: 12pt
lang: de
---


# Zusammenfassung {.unnumbered}

\noindent 
Dieses Dokument beschreibt die Datenaufbereitung der Datei 'Dataset 71.txt' und stellt sicher, dass die Ergebnisse replizierbar sind. Alle Schritte werden mit R durchgeführt.

\newpage    
# Datenbeschreibung

Über den Datensatz ist mir nur wenig bekannt. Es gibt 21 Variablen, die Items einer Umfrage darstellen. Die Antwortmöglichkeiten sind auf einer fünfstufigen Likert-Skala angegeben.


# Datenaufbereitung

## Vorbereitung 
Zunächst werden die im verwendeten R-Pakete geladen. Hierzu verwende ich das Paket `pacman`. Sollte dies auf dem verwendeten PC nicht installiert sein, wird es mit der ersten der folgenden Zeile installiert. Die zweite Zeile lädt die benötigten Pakete und die dritte bereinigt den aktuellen Arbeitsbereich. Schließlich wird das Arbeitsverzeichnisses festgelegt. 

```{r, echo=TRUE, message=FALSE}
if (!require(pacman)) install.packages("pacman")
pacman::p_load(tidyverse, janitor, psych, tinytable, ggstats,
               modelsummary, knitr, kableExtra, labelled)
rm(list = ls())
setwd("~/Dropbox/hsf/24-ss/ewa/ewa_papers/read_in_71/")
```  

## Datenimport

Mit der Funktion `read.delim` wird die Datei "Dataset 71.txt" in R eingelesen. @tbl-df_raw_glimpse zeigt einen Ausschnitt des Rohdatensatzes.

```{r, echo=TRUE, message=FALSE, output = FALSE, warning=FALSE}
df_raw <- read.delim("Dataset 71.txt")
```

```{r include=FALSE}
head_df_raw <- head(df_raw)
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-df_raw_glimpse
#| echo: false
#| tbl-cap: "Ausschnitt des Rohdatensatz"

tt(head_df_raw[,1:11])

# knitr::kable( glimpse_df_raw, output = "latex", booktabs = TRUE)
```


## Datenexploration

Im Folgenden werde ich die Daten etwas untersuchen. Ziel ist das Auffinden von Datenfehlern, um diese im Weiteren zu berücksichtigen und gegebenenfalls zu bereinigen. 
Zunächst gibt es Festzuhalten, dass folgende Werte im Datensatz vorhanden sind: `r unique(unlist(df_raw))`. Ohne die Variable `ID`, welche offensichtlich eine laufende Nummer von 1 bis 70 ist, sind folgende Werte enthalten: 
`r df_raw |> 
  select(-ID) |> 
  as.matrix() |> 
  as.vector() |> 
  unique()`. 
Das ist seltsam. Eigentlich sollten hier, entsprechend der Likertskala, nur Werte von 1 bis 5 enthalten sein.  

@tbl-df_raw_skim zeigt einige deskriptive Statistiken. Hier fällt auf, dass `NaN` enthalten sind und einige ungewöhnliche Werte die außerhalb der Werte 1 bis 5 liegen.^[@tbl-df_raw_skim wird mit Hilfe der Funktion `datasummary_skim()` erstellt. Diese ist Teil des Paket `modelsummary` [@modelsummary2022].] @tbl-df_long zeigt an, in wie vielen verschiedenen Items die bestimmte Beobachtungen vorkommen.^[@tbl-df_raw_long wird mit Hilfe der Funktion `tabyl()` erstellt. Diese ist Teil des Paket `janitor` [@janitor2023].] Dies gilt es im Folgenden zu bereinigen.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
#| label: tbl-df_raw_unique
#| echo: false
#| tbl-cap: "Unterschiedliche Werte in den Variablen"

uv <- df_raw |> 
  select(-ID) |> 
  as.matrix() |> 
  as.vector() |> 
  unique()

uv_item <- df_raw |> 
  select(-ID) |> 
  map(~ unique(c(., NaN)) |>  sort()) |> 
  enframe(name = "Attribute", value = "Values") |>  
  tibble()

# Create and display the table using kable
uv_item |> 
   kable("latex", booktabs = TRUE, longtable = TRUE) 
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#| label: tbl-df_long
#| echo: false
#| tbl-cap: "Häufigkeitstabelle der unterschiedlichen Werte (pro item)"
#| apa-note: "Die Tabelle zeigt an, in wie vielen Items die jeweiligen Werte vorkommen."

long <- df_raw  |> 
  pivot_longer(!ID, names_to = "item", values_to = "count") |> 
  distinct(item, count) |> 
  arrange(item, count) 

long$count |> 
  tabyl() |> 
  kable("latex", booktabs = TRUE) 

```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#| label: tbl-df_raw_skim
#| echo: false
#| tbl-cap: "Deskriptive Statistiken zum Rohdatensatz"

datasummary_skim(df_raw, output = "latex")
```



## Datenbereinigung

Zunächst nehme ich einige kosmetische Bereinigungen vor. Hierbei betitele ich die Variblennamen ensprechend den Konventionen ohne Lehrzeichen oder Punkte und ich vermeide Großbuchstaben. Dies geschieht mit der Funktion `clean_names`. Darüber hinaus werden die `NaN` Werte durch `NA` ersetzt und die Beobachtungen gelöscht die in allen items ausschließlich missings enthalten. 

```{r, echo=TRUE, message=FALSE}
df_cosmetic <- df_raw |>
  clean_names() |>
  as_tibble() |>
  # Ersetzen von NaN-Werten durch NA
  mutate(across(everything(), ~ if_else(is.nan(.), NA, .))) |>
  #Entfernen von Zeilen, bei denen alle "item_"-Spalten NA sind
  rowwise() |> 
  filter(!all(across(starts_with("item_"), ~ is.na(.)))) |> 
  ungroup()
```  

Wie bereits erwähnt gibt es einige Missings (`NA`) sowie ungewöhnliche Werte die außerhalb der Likertskala sind. Also nicht im Wertebereich 1 bis 5. Diese gilt es zu identifizieren. Da ich die genauen Gründe hierfür nicht kenne, werde ich hierzu verschiedene Variablen erzeugt. Die genaueren Beschreibungen zu den Variablen befinden sich als Kommentar im Folgenden Code-Ausschnitt: 

```{r, echo=TRUE, message=FALSE}
df <- df_cosmetic |>
  rowwise() |>
  # Berechnung des größten absoluten Werts in "item_"-Spalten für jede Zeile
  mutate(outlier = max(abs(c_across(starts_with("item_"))), na.rm = TRUE)) |>
  # Markieren, ob ein Ausreißer (> 5 oder gleich 0) vorhanden ist
  mutate(has_outlier = if_else(outlier > 5 | outlier == 0, TRUE, FALSE)) |>
  # Zählen der Werte, die größer als 5 sind, für jede Zeile
  mutate(count_larger_5 = 
           sum( c_across(starts_with("item_")) > 5 | 
                c_across(starts_with("item_")) == 0, na.rm = TRUE)) |> 
  # Zählen der Tippfehler (11, 22, 33, 44, 55) für jede Zeile
  mutate(count_typos = sum(c_across(starts_with("item_")) %in% 
                             c(11, 22, 33, 44, 55), na.rm = TRUE)) |> 
  # Markieren, ob mehr Werte größer als 5 sind als Tippfehler
  mutate(has_larger_5_notypos = (count_typos < count_larger_5)) |> 
  # Markieren, ob Tippfehler vorhanden sind
  mutate(has_typos = count_typos > 0 ) |>
  # Markieren, ob NA-Werte in "item_"-Spalten vorhanden sind
  mutate(has_nas = if_else(anyNA(pick(starts_with("item_"))), TRUE, FALSE)) |>
  # Markieren, ob eine Zeile vollständig ist (keine Ausreißer und keine NAs)
  mutate(complete = (has_outlier == FALSE & has_nas == FALSE)) |> 
  ungroup()
```  

Die Variablen `has_typos`, `has_nas`, `has_larger_5_notypos` und `has_outlier` indizieren nun, ob und welche Probleme in der jeweiligen Beobachtung vorliegen. Die Variablen sind wie folgt definiert:

- `has_nas`: Ist `TRUE`, wenn mindestens eine Beobachtung ein `NA` ist. 
- `has_typos`: Ist `TRUE`, wenn mindestens eine Beobachtung die Werte 11, 22, 33, 44, oder 55 aufweist.
- `has_outlier`: Ist `TRUE`, wenn mindestens eine Beobachtung die Werte in absoluten Zahlen größer als 5 ist
- `has_larger_5_notypos`: Ist `TRUE`, wenn mindestens eine Beobachtung die Werte in absoluten Zahlen größer als 5 ist und diese Zahl(en) nicht 11, 22, 33, 44, oder 55 ist. 

Die Werte 11, 22, 33, 44, oder 55 könnten besonders sein, denn hier könnte man vermuten, dass hier schlicht ein Tippfehler vorliegt. Also die Zahl versehentlich doppelt eingegeben wurde. Dies werde ich später versuchen, zu berücksichtigen und zu bereinigen.

## Datensatzerstellung

In diesem Schritt erstelle ich Datensätze die ich zur Analyse verwenden kann. Hierbei werde ich zwei verschiedene Datensätze erstellen. Einen Datensatz in dem ich ausschließlich Beobachtungen berücksichtige die scheinbar frei von Fehleingaben und fehlenenden Werten ist. Dieser Datensatz bezeichne ich mit `df_complete`. Darüber hinaus ist zu berücksichtigen, dass ich alle Variablen, der Likertskala entsprechend, als Faktorvariablen abspeichere und ich versehe sie mit einem entsprechenden Label.

```{r, echo=TRUE, message=FALSE}
# Labels definieren
likert_levels <- c(
  "Stimme überhaupt nicht zu",
  "Stimme nicht zu",
  "Neutral",
  "Stimme zu",
  "Stimme voll und ganz zu"
)

# Faktorisierung der Items und hinzufügen eines Labels
df_chr <- df |> 
  mutate(across(starts_with("item_"), 
                ~ case_when(
                  . == 1 ~ "Stimme überhaupt nicht zu",
                  . == 2 ~ "Stimme nicht zu",
                  . == 3 ~ "Neutral",
                  . == 4 ~ "Stimme zu",
                  . == 5 ~ "Stimme voll und ganz zu",
                  TRUE ~ as.character(.)
                ))) |> 
  mutate(across(starts_with("item_"), ~ factor(.x, levels = likert_levels)))

df_complete <- df_chr |> 
  filter(complete == TRUE) 
  
```  
Der Datensatz `df_complete` hat `r nrow(df_complete)` Beobachtungen.

Der zweite Datensatz ist `df_cleaned` betitelt. Hierbei unterstelle ich dass es sich bei den Eingaben mit den Werten 11, 22, 33, 44, oder 55 um Tippfehler handelt. Diese rekodiere ich  entsprechend in 1, 2, 3, 4 und 5 um. Alle übrigen werte außerhalb des Wertebereichs 1 bis 5 bezeichne ich als `NA`. 


```{r, echo=TRUE, message=FALSE}
df_cleaned <- df |> 
  # Ersetzen von bestimmten Werten (11, 22, 33, 44, 55) in "item_"-Spalten
  mutate(across(starts_with("item_"), ~ case_when(
    . == 11 ~ 1,
    . == 22 ~ 2,
    . == 33 ~ 3,
    . == 44 ~ 4,
    . == 55 ~ 5,
    TRUE ~ .
  ))) |> 
  # Ersetzen von Werten größer als 5 durch NA in "item_"-Spalten
  mutate(across(starts_with("item_"), ~ if_else(. > 5 | . == 0, NA, .))) |> 
  mutate(across(starts_with("item_"), 
                ~ case_when(
                  . == 1 ~ "Stimme überhaupt nicht zu",
                  . == 2 ~ "Stimme nicht zu",
                  . == 3 ~ "Neutral",
                  . == 4 ~ "Stimme zu",
                  . == 5 ~ "Stimme voll und ganz zu",
                  TRUE ~ as.character(.)
                ))) |> 
  mutate(across(starts_with("item_"), ~ factor(.x, levels = likert_levels)))

```  

Der Datensatz `df_cleaned` hat `r nrow(df_cleaned)` Beobachtungen.

Schließlich speichere ich die aktuelle Arbeitsumgebung in einer .RData-Datei:

```{r, echo=TRUE, message=FALSE}
save.image("data_71.RData")
```

# Auswertung

@fig-df_com_gglik und @fig-df_cle_gglik zeigen die Verteilung der Antworten. Ersterer verwendet den Datensatz `df_complete`, also dem Datensatz bei dem nur die Befragten berücksichtigt wurden bei denen keinerlei auffälliges gefunden wurde. Zweitere Abbildung verwendet den Datensatz `df_cleaned` bei dem einige Bereinigungen durchgeführt wurden und einige Fragen nicht verfügbar waren.^[Beide Abbildungen werden mit der function `gglikert` erstellt. Diese ist Teil des `ggstats` Paket [@ggstats2024].]

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-df_com_gglik
#| echo: false
#| fig-cap: "Antwortverteilung zu den gestellten Fragen (df_complete)"

gglikert(df_complete, include = starts_with("item_"))
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#| label: fig-df_cle_gglik
#| echo: false
#| fig-cap: "Antwortverteilung zu den gestellten Fragen (df_cleaned)"

gglikert(df_cleaned, include = starts_with("item_"))
```

```{r include=FALSE}
knitr::purl("doc_read_in_71.qmd")
```




\newpage
# Literaturverzeichnis {.unnumbered}

::: {#refs}
:::
